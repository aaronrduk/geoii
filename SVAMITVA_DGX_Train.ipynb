{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# SVAMITVA ‚Äî DGX Training Pipeline (MAPC Sub-Maps)\n",
    "\n",
    "**Target:** DGX Server ‚Äî single GPU with the most free VRAM.\n",
    "**DATA path:** `/jupyter/sods.user04/DATA/MAPC` (pre-clipped 512√ó512 sub-maps)\n",
    "**Checkpoints:** `/jupyter/sods.user04/check/MAP_best.pt` / `MAP_latest.pt`\n",
    "\n",
    "Trains each sub-map (MAP1.1, MAP1.2, ‚Ä¶) individually in sequence. One global checkpoint. Asks permission before moving to next parent map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1 ‚Äî Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ‚îÄ‚îÄ GPU: pick the one with the most free VRAM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def get_best_gpu():\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE, encoding='utf-8'\n",
    "    )\n",
    "    free_memories = [int(x) for x in result.stdout.strip().split('\\n')]\n",
    "    best_idx = max(range(len(free_memories)), key=lambda i: free_memories[i])\n",
    "    free_gb = free_memories[best_idx] / 1024\n",
    "    print(f\"  GPU {best_idx} selected ‚Äî {free_gb:.1f} GB free (max of {len(free_memories)} GPUs)\")\n",
    "    return str(best_idx)\n",
    "\n",
    "gpu_id = get_best_gpu()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ‚îÄ‚îÄ Project root discovery ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "for parent in [NOTEBOOK_DIR] + list(NOTEBOOK_DIR.parents):\n",
    "    if (parent / \"models\").exists() and (parent / \"models/__init__.py\").exists():\n",
    "        NOTEBOOK_DIR = parent\n",
    "        break\n",
    "if str(NOTEBOOK_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(NOTEBOOK_DIR))\n",
    "\n",
    "# ‚îÄ‚îÄ Directories ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "DATA_DIR = Path(\"/jupyter/sods.user04/DATA/MAPC\")\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = Path(\"/DATA/MAPC\")\n",
    "CKPT_DIR = Path(\"/jupyter/sods.user04/check\")\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(NOTEBOOK_DIR / \"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "CONFIG = dict(\n",
    "    backbone              = \"resnet50\",\n",
    "    pretrained            = True,\n",
    "    image_size            = 512,\n",
    "    batch_size            = 16,          # Adjust per GPU VRAM (16 for 24GB, 32 for 40GB+)\n",
    "    epochs_per_map        = 15,          # More epochs for better convergence\n",
    "    learning_rate         = 3e-4,\n",
    "    weight_decay          = 1e-4,\n",
    "    num_workers           = 4,\n",
    "    mixed_precision       = True,\n",
    "    gradient_clip         = 0.5,\n",
    "    building_weight       = 1.0,\n",
    "    roof_weight           = 0.5,\n",
    "    road_weight           = 1.0,\n",
    "    waterbody_weight      = 1.2,\n",
    "    road_centerline_weight= 1.0,\n",
    "    waterbody_line_weight = 1.2,\n",
    "    waterbody_point_weight= 1.5,\n",
    "    utility_line_weight   = 1.2,\n",
    "    utility_poly_weight   = 1.3,\n",
    "    bridge_weight         = 1.5,\n",
    "    railway_weight        = 1.3,\n",
    ")\n",
    "\n",
    "TARGET_KEYS = [\n",
    "    \"building_mask\", \"road_mask\", \"road_centerline_mask\",\n",
    "    \"waterbody_mask\", \"waterbody_line_mask\", \"waterbody_point_mask\",\n",
    "    \"utility_line_mask\", \"utility_poly_mask\",\n",
    "    \"bridge_mask\", \"railway_mask\", \"roof_type_mask\",\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Setup | GPU: {gpu_id} | DATA: {DATA_DIR} (exists={DATA_DIR.exists()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 ‚Äî Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "try:\n",
    "    from torch.amp import GradScaler, autocast\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "from models.losses import MultiTaskLoss\n",
    "from training.metrics import MetricTracker\n",
    "from data.dataset import SvamitvaDataset\n",
    "from data.augmentation import get_train_transforms\n",
    "\n",
    "# --- Single global checkpoint files ---\n",
    "BEST_CKPT = CKPT_DIR / \"MAP_best.pt\"\n",
    "LATEST_CKPT = CKPT_DIR / \"MAP_latest.pt\"\n",
    "\n",
    "\n",
    "def move_targets(batch):\n",
    "    return {k: batch[k].to(device, non_blocking=True) for k in TARGET_KEYS if k in batch}\n",
    "\n",
    "\n",
    "def build_model(load_from: Path = None):\n",
    "    m = FeatureExtractor(\n",
    "        backbone=CONFIG[\"backbone\"],\n",
    "        pretrained=CONFIG[\"pretrained\"],\n",
    "        num_roof_classes=5,\n",
    "    )\n",
    "\n",
    "    if load_from and load_from.exists():\n",
    "        state = torch.load(load_from, map_location=\"cpu\", weights_only=False)\n",
    "        weights = state.get(\"model\") or state.get(\"model_state_dict\") or state\n",
    "        m.load_state_dict(weights, strict=False)\n",
    "        print(f\"  Loaded weights from: {load_from.name}\")\n",
    "\n",
    "    return m.to(device)\n",
    "\n",
    "\n",
    "def train_submap(sub_name: str, model_w, optimizer, scheduler, scaler, best_iou):\n",
    "    \"\"\"Train one sub-map (e.g. MAP1.42) with AMP on GPU.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    sub_dir = DATA_DIR / sub_name\n",
    "    if not sub_dir.exists():\n",
    "        print(f\"  [SKIP] {sub_dir} not found\")\n",
    "        return model_w, optimizer, scheduler, best_iou, scaler\n",
    "\n",
    "    ds = SvamitvaDataset(\n",
    "        root_dir=DATA_DIR,\n",
    "        image_size=CONFIG[\"image_size\"],\n",
    "        transform=get_train_transforms(CONFIG[\"image_size\"]),\n",
    "        mode=\"train\",\n",
    "    )\n",
    "    ds.samples = [s for s in ds.samples if s[\"map_name\"] == sub_name]\n",
    "\n",
    "    if not ds.samples:\n",
    "        print(f\"  [SKIP] {sub_name}: 0 tiles\")\n",
    "        return model_w, optimizer, scheduler, best_iou, scaler\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds, batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=True, num_workers=CONFIG[\"num_workers\"],\n",
    "        pin_memory=True, drop_last=False,\n",
    "    )\n",
    "\n",
    "    loss_fn = MultiTaskLoss(\n",
    "        **{k: v for k, v in CONFIG.items() if k.endswith(\"_weight\")}\n",
    "    ).to(device)\n",
    "\n",
    "    use_amp = CONFIG[\"mixed_precision\"] and device.type == \"cuda\"\n",
    "\n",
    "    for epoch in range(1, CONFIG[\"epochs_per_map\"] + 1):\n",
    "        model_w.train()\n",
    "        tracker = MetricTracker()\n",
    "        run_loss, n_steps, t0 = 0.0, 0, time.time()\n",
    "\n",
    "        try:\n",
    "            for batch in loader:\n",
    "                images = batch[\"image\"].to(device, non_blocking=True)\n",
    "                targets = move_targets(batch)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                if not torch.isfinite(images).all():\n",
    "                    continue\n",
    "\n",
    "                if use_amp:\n",
    "                    with autocast(device_type=\"cuda\", enabled=True):\n",
    "                        preds = model_w(images)\n",
    "                        total_loss, _ = loss_fn(preds, targets)\n",
    "                    if not torch.isfinite(total_loss):\n",
    "                        continue\n",
    "                    scaler.scale(total_loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    nn.utils.clip_grad_norm_(model_w.parameters(), CONFIG[\"gradient_clip\"])\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    preds = model_w(images)\n",
    "                    total_loss, _ = loss_fn(preds, targets)\n",
    "                    if not torch.isfinite(total_loss):\n",
    "                        continue\n",
    "                    total_loss.backward()\n",
    "                    if CONFIG[\"gradient_clip\"] > 0:\n",
    "                        nn.utils.clip_grad_norm_(model_w.parameters(), CONFIG[\"gradient_clip\"])\n",
    "                    optimizer.step()\n",
    "\n",
    "                run_loss += total_loss.item()\n",
    "                tracker.update(preds, targets)\n",
    "                n_steps += 1\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n  [INTERRUPT] Saving emergency checkpoint...\")\n",
    "            save_inference_checkpoint(model_w, LATEST_CKPT)\n",
    "            raise\n",
    "\n",
    "        scheduler.step()\n",
    "        metrics = tracker.compute()\n",
    "        avg_loss = run_loss / max(n_steps, 1)\n",
    "        avg_iou = metrics.get(\"avg_iou\", 0.0)\n",
    "\n",
    "        print(\n",
    "            f\"    Epoch {epoch}/{CONFIG['epochs_per_map']} | \"\n",
    "            f\"loss: {avg_loss:.4f} | iou: {avg_iou:.4f} | {time.time()-t0:.0f}s\"\n",
    "        )\n",
    "\n",
    "        # Always save latest\n",
    "        save_training_checkpoint(model_w, optimizer, epoch, sub_name, best_iou, LATEST_CKPT)\n",
    "        save_inference_checkpoint(model_w, LATEST_CKPT)\n",
    "\n",
    "        # Update best if improved\n",
    "        if avg_iou > best_iou:\n",
    "            best_iou = avg_iou\n",
    "            save_training_checkpoint(model_w, optimizer, epoch, sub_name, best_iou, BEST_CKPT)\n",
    "            save_inference_checkpoint(model_w, BEST_CKPT)\n",
    "            print(f\"    \\u2192 New best! IoU = {best_iou:.4f}\")\n",
    "\n",
    "    return model_w, optimizer, scheduler, best_iou, scaler\n",
    "\n",
    "\n",
    "def train_parent_map(parent_map: str, resume_from: Path = None):\n",
    "    \"\"\"Train each sub-map (MAP1.1, MAP1.2, ...) individually in sequence.\n",
    "    All checkpoints go to MAP_best.pt / MAP_latest.pt.\"\"\"\n",
    "\n",
    "    sub_maps = sorted(\n",
    "        [d.name for d in DATA_DIR.iterdir()\n",
    "         if d.is_dir() and d.name.startswith(parent_map + \".\")],\n",
    "        key=lambda n: int(n.split(\".\")[-1])\n",
    "    )\n",
    "    if not sub_maps:\n",
    "        print(f\"[SKIP] No sub-maps for {parent_map}\")\n",
    "        return resume_from\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Parent map : {parent_map}  ({len(sub_maps)} sub-maps)\")\n",
    "    print(f\"  Sub-maps   : {sub_maps[0]} \\u2192 {sub_maps[-1]}\")\n",
    "    print(f\"  Resume     : {resume_from.name if resume_from and resume_from.exists() else 'SCRATCH'}\")\n",
    "    print(f\"  Saving to  : {BEST_CKPT.name} / {LATEST_CKPT.name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    model_w = build_model(load_from=resume_from)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model_w.parameters(),\n",
    "        lr=CONFIG[\"learning_rate\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"]\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=CONFIG[\"epochs_per_map\"], eta_min=1e-6\n",
    "    )\n",
    "    scaler = GradScaler(enabled=CONFIG[\"mixed_precision\"] and device.type == \"cuda\")\n",
    "\n",
    "    # Load best_iou from existing checkpoint if resuming\n",
    "    best_iou = 0.0\n",
    "    if resume_from and resume_from.exists():\n",
    "        try:\n",
    "            train_ckpt = resume_from.parent / (resume_from.stem + \".train.pt\")\n",
    "            if train_ckpt.exists():\n",
    "                st = torch.load(train_ckpt, map_location=\"cpu\", weights_only=False)\n",
    "                best_iou = st.get(\"best_iou\", 0.0)\n",
    "                print(f\"  Resuming with best_iou = {best_iou:.4f}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        for i, sub_name in enumerate(sub_maps, 1):\n",
    "            print(f\"\\n  [{i}/{len(sub_maps)}] Training {sub_name} \\u2026\")\n",
    "            model_w, optimizer, scheduler, best_iou, scaler = train_submap(\n",
    "                sub_name, model_w, optimizer, scheduler, scaler, best_iou\n",
    "            )\n",
    "    except (Exception, KeyboardInterrupt) as e:\n",
    "        print(f\"\\n  \\u26a0\\ufe0f EMERGENCY SAVE: {e}\")\n",
    "        save_inference_checkpoint(model_w, CKPT_DIR / \"MAP_crash_backup.pt\")\n",
    "        raise\n",
    "\n",
    "    print(f\"\\n  \\u2705 {parent_map} complete \\u2014 best IoU: {best_iou:.4f}\")\n",
    "    return BEST_CKPT if BEST_CKPT.exists() else LATEST_CKPT\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CHECKPOINT HELPERS\n",
    "# ----------------------------\n",
    "\n",
    "def save_training_checkpoint(model, optimizer, epoch, map_name, best_iou, path):\n",
    "    inner = model.module if hasattr(model, \"module\") else model\n",
    "    train_path = path.parent / (path.stem + \".train.pt\")\n",
    "    ckpt = {\n",
    "        \"model_state_dict\": inner.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"map_name\": map_name,\n",
    "        \"best_iou\": best_iou,\n",
    "    }\n",
    "    torch.save(ckpt, train_path)\n",
    "\n",
    "\n",
    "def save_inference_checkpoint(model, path):\n",
    "    inner = model.module if hasattr(model, \"module\") else model\n",
    "    torch.save(\n",
    "        {k: v.cpu() for k, v in inner.state_dict().items()},\n",
    "        path\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Training engine ready \\u2713\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 ‚Äî Execute Training\n",
    "Trains each sub-map individually. One global `MAP_best.pt` / `MAP_latest.pt`. Asks permission before next parent map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover sub-map folders and group by parent map\n",
    "sub_folders = sorted([\n",
    "    d.name for d in DATA_DIR.iterdir()\n",
    "    if d.is_dir() and d.name.startswith(\"MAP\") and \".\" in d.name\n",
    "])\n",
    "\n",
    "parent_maps = []\n",
    "seen = set()\n",
    "for name in sub_folders:\n",
    "    parent = name.split(\".\")[0]\n",
    "    if parent not in seen:\n",
    "        seen.add(parent)\n",
    "        parent_maps.append(parent)\n",
    "parent_maps.sort()\n",
    "\n",
    "sub_counts = {p: sum(1 for n in sub_folders if n.startswith(p + \".\")) for p in parent_maps}\n",
    "print(f\"üöÄ Found {len(parent_maps)} parent maps: {[f'{p} ({sub_counts[p]} sub-maps)' for p in parent_maps]}\")\n",
    "print(f\"Checkpoints: {BEST_CKPT} / {LATEST_CKPT}\\n\")\n",
    "\n",
    "prev_ckpt = BEST_CKPT if BEST_CKPT.exists() else None\n",
    "\n",
    "for idx, p_name in enumerate(parent_maps):\n",
    "    # Ask permission before each parent map (except the first)\n",
    "    if idx > 0:\n",
    "        answer = input(f\"\\nüîî Continue to {p_name} ({sub_counts[p_name]} sub-maps)? [yes/no]: \").strip().lower()\n",
    "        if answer not in (\"yes\", \"y\"):\n",
    "            print(f\"‚õî Stopped before {p_name}. Checkpoints saved.\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n‚è≥ Training {p_name} ({sub_counts[p_name]} sub-maps individually)...\")\n",
    "    ckpt = train_parent_map(p_name, resume_from=prev_ckpt)\n",
    "    if ckpt and Path(ckpt).exists():\n",
    "        prev_ckpt = ckpt\n",
    "    else:\n",
    "        print(f\"‚ùå {p_name} failed\")\n",
    "\n",
    "print(\"\\n*** DGX TRAINING COMPLETE ***\")\n",
    "print(f\"Best checkpoint : {BEST_CKPT}  (exists={BEST_CKPT.exists()})\")\n",
    "print(f\"Latest checkpoint: {LATEST_CKPT}  (exists={LATEST_CKPT.exists()})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}