{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# SVAMITVA \u2014 Local Mac Training Pipeline (MAPC Sub-Maps)\n",
    "\n",
    "**Target:** Apple Silicon MPS or CPU.  \n",
    "**DATA path:** `/Users/aaronr/Desktop/DATA/MAPC` (pre-clipped 512\u00d7512 sub-maps)\n",
    "\n",
    "Discovers `MAP1.*`, `MAP2.*`, etc. sub-maps, groups by parent map, and trains sequentially for rapid local prototyping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1 \u2014 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Silicon GPU (MPS) \u2705\n",
      "DATA: /Users/aaronr/Desktop/DATA/MAPC  (exists=True)\n",
      "\n",
      "Setup complete \u2713\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "if str(NOTEBOOK_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(NOTEBOOK_DIR))\n",
    "\n",
    "DATA_DIR = Path(\"/Users/aaronr/Desktop/DATA/MAPC\")\n",
    "CKPT_DIR = NOTEBOOK_DIR / \"checkpoints\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(NOTEBOOK_DIR / \"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Apple Silicon GPU (MPS) \u2705\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU \u26a0\ufe0f\")\n",
    "\n",
    "print(f\"DATA: {DATA_DIR}  (exists={DATA_DIR.exists()})\")\n",
    "\n",
    "CONFIG = dict(\n",
    "    backbone              = \"resnet34\",  # Lighter for local test\n",
    "    pretrained            = True,\n",
    "    image_size            = 256,         # Smaller for local test\n",
    "    batch_size            = 4,\n",
    "    epochs_per_map        = 5,           # Less epochs for local test\n",
    "    learning_rate         = 2e-4,\n",
    "    weight_decay          = 1e-4,\n",
    "    num_workers           = 0,\n",
    "    mixed_precision       = False,       # MPS AMP unstable\n",
    "    gradient_clip         = 1.0,\n",
    "    building_weight       = 1.0,\n",
    "    roof_weight           = 0.5,\n",
    "    road_weight           = 0.8,\n",
    "    waterbody_weight      = 0.8,\n",
    "    road_centerline_weight= 0.7,\n",
    "    waterbody_line_weight = 0.7,\n",
    "    waterbody_point_weight= 0.9,\n",
    "    utility_line_weight   = 0.7,\n",
    "    utility_poly_weight   = 0.8,\n",
    "    bridge_weight         = 1.0,\n",
    "    railway_weight        = 0.9,\n",
    ")\n",
    "\n",
    "TARGET_KEYS = [\n",
    "    \"building_mask\", \"road_mask\", \"road_centerline_mask\",\n",
    "    \"waterbody_mask\", \"waterbody_line_mask\", \"waterbody_point_mask\",\n",
    "    \"utility_line_mask\", \"utility_poly_mask\",\n",
    "    \"bridge_mask\", \"railway_mask\", \"roof_type_mask\",\n",
    "]\n",
    "\n",
    "print(\"\\nSetup complete \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dce524",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cell2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 \u2014 Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training engine ready \u2713\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.feature_extractor import FeatureExtractor\n",
    "from models.losses import MultiTaskLoss\n",
    "from training.metrics import MetricTracker\n",
    "from data.dataset import SvamitvaDataset\n",
    "from data.augmentation import get_train_transforms\n",
    "\n",
    "# --- Single global checkpoint files ---\n",
    "BEST_CKPT = CKPT_DIR / \"MAP_best.pt\"\n",
    "LATEST_CKPT = CKPT_DIR / \"MAP_latest.pt\"\n",
    "\n",
    "\n",
    "def move_targets(batch):\n",
    "    return {k: batch[k].to(device) for k in TARGET_KEYS if k in batch}\n",
    "\n",
    "\n",
    "def build_model(load_from: Path = None):\n",
    "    m = FeatureExtractor(\n",
    "        backbone=CONFIG[\"backbone\"],\n",
    "        pretrained=CONFIG[\"pretrained\"],\n",
    "        num_roof_classes=5,\n",
    "    )\n",
    "\n",
    "    if load_from and load_from.exists():\n",
    "        state = torch.load(load_from, map_location=\"cpu\", weights_only=False)\n",
    "        weights = state.get(\"model\") or state.get(\"model_state_dict\") or state\n",
    "        m.load_state_dict(weights, strict=False)\n",
    "        print(f\"  Loaded weights from: {load_from.name}\")\n",
    "\n",
    "    return m.to(device)\n",
    "\n",
    "\n",
    "def train_submap(sub_name: str, model_w, optimizer, scheduler, scaler_state, best_iou):\n",
    "    \"\"\"Train one sub-map (e.g. MAP1.42). Returns (model, optimizer, scheduler, best_iou).\"\"\"\n",
    "    sub_dir = DATA_DIR / sub_name\n",
    "    if not sub_dir.exists():\n",
    "        print(f\"  [SKIP] {sub_dir} not found\")\n",
    "        return model_w, optimizer, scheduler, best_iou, scaler_state\n",
    "\n",
    "    ds = SvamitvaDataset(\n",
    "        root_dir=DATA_DIR,\n",
    "        image_size=CONFIG[\"image_size\"],\n",
    "        transform=get_train_transforms(CONFIG[\"image_size\"]),\n",
    "        mode=\"train\",\n",
    "    )\n",
    "    ds.samples = [s for s in ds.samples if s[\"map_name\"] == sub_name]\n",
    "\n",
    "    if not ds.samples:\n",
    "        print(f\"  [SKIP] {sub_name}: 0 tiles\")\n",
    "        return model_w, optimizer, scheduler, best_iou, scaler_state\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds, batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=True, num_workers=0, pin_memory=False\n",
    "    )\n",
    "\n",
    "    loss_fn = MultiTaskLoss(\n",
    "        **{k: v for k, v in CONFIG.items() if k.endswith(\"_weight\")}\n",
    "    ).to(device)\n",
    "\n",
    "    for epoch in range(1, CONFIG[\"epochs_per_map\"] + 1):\n",
    "        model_w.train()\n",
    "        tracker = MetricTracker()\n",
    "        run_loss, n_steps, t0 = 0.0, 0, time.time()\n",
    "\n",
    "        try:\n",
    "            for batch in loader:\n",
    "                images = batch[\"image\"].to(device)\n",
    "                targets = move_targets(batch)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                if not torch.isfinite(images).all():\n",
    "                    continue\n",
    "\n",
    "                preds = model_w(images)\n",
    "                total_loss, _ = loss_fn(preds, targets)\n",
    "\n",
    "                if not torch.isfinite(total_loss):\n",
    "                    continue\n",
    "\n",
    "                total_loss.backward()\n",
    "                if CONFIG[\"gradient_clip\"] > 0:\n",
    "                    nn.utils.clip_grad_norm_(model_w.parameters(), CONFIG[\"gradient_clip\"])\n",
    "                optimizer.step()\n",
    "\n",
    "                run_loss += total_loss.item()\n",
    "                tracker.update(preds, targets)\n",
    "                n_steps += 1\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n  [INTERRUPT] Saving emergency checkpoint...\")\n",
    "            save_inference_checkpoint(model_w, LATEST_CKPT)\n",
    "            raise\n",
    "\n",
    "        scheduler.step()\n",
    "        metrics = tracker.compute()\n",
    "        avg_loss = run_loss / max(n_steps, 1)\n",
    "        avg_iou = metrics.get(\"avg_iou\", 0.0)\n",
    "\n",
    "        print(\n",
    "            f\"    Epoch {epoch}/{CONFIG['epochs_per_map']} | \"\n",
    "            f\"loss: {avg_loss:.4f} | iou: {avg_iou:.4f} | {time.time()-t0:.0f}s\"\n",
    "        )\n",
    "\n",
    "        # Always save latest\n",
    "        save_training_checkpoint(model_w, optimizer, epoch, sub_name, best_iou, LATEST_CKPT)\n",
    "        save_inference_checkpoint(model_w, LATEST_CKPT)\n",
    "\n",
    "        # Update best if improved\n",
    "        if avg_iou > best_iou:\n",
    "            best_iou = avg_iou\n",
    "            save_training_checkpoint(model_w, optimizer, epoch, sub_name, best_iou, BEST_CKPT)\n",
    "            save_inference_checkpoint(model_w, BEST_CKPT)\n",
    "            print(f\"    \u2192 New best! IoU = {best_iou:.4f}\")\n",
    "\n",
    "    return model_w, optimizer, scheduler, best_iou, None\n",
    "\n",
    "\n",
    "def train_parent_map(parent_map: str, resume_from: Path = None):\n",
    "    \"\"\"Train each sub-map (MAP1.1, MAP1.2, \u2026) individually in sequence.\n",
    "    All checkpoints go to MAP_best.pt / MAP_latest.pt.\"\"\"\n",
    "\n",
    "    sub_maps = sorted(\n",
    "        [d.name for d in DATA_DIR.iterdir()\n",
    "         if d.is_dir() and d.name.startswith(parent_map + \".\")],\n",
    "        key=lambda n: int(n.split(\".\")[-1])  # sort numerically: 1,2,\u2026,3180\n",
    "    )\n",
    "    if not sub_maps:\n",
    "        print(f\"[SKIP] No sub-maps for {parent_map}\")\n",
    "        return resume_from\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Parent map : {parent_map}  ({len(sub_maps)} sub-maps)\")\n",
    "    print(f\"  Sub-maps   : {sub_maps[0]} \u2192 {sub_maps[-1]}\")\n",
    "    print(f\"  Resume     : {resume_from.name if resume_from and resume_from.exists() else 'SCRATCH'}\")\n",
    "    print(f\"  Saving to  : {BEST_CKPT.name} / {LATEST_CKPT.name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    model_w = build_model(load_from=resume_from)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model_w.parameters(),\n",
    "        lr=CONFIG[\"learning_rate\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"]\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=CONFIG[\"epochs_per_map\"], eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # Load best_iou from existing checkpoint if resuming\n",
    "    best_iou = 0.0\n",
    "    if resume_from and resume_from.exists():\n",
    "        try:\n",
    "            st = torch.load(resume_from.with_suffix(\".train.pt\"), map_location=\"cpu\")\n",
    "            best_iou = st.get(\"best_iou\", 0.0)\n",
    "            print(f\"  Resuming with best_iou = {best_iou:.4f}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for i, sub_name in enumerate(sub_maps, 1):\n",
    "        print(f\"\\n  [{i}/{len(sub_maps)}] Training {sub_name} \u2026\")\n",
    "        model_w, optimizer, scheduler, best_iou, _ = train_submap(\n",
    "            sub_name, model_w, optimizer, scheduler, None, best_iou\n",
    "        )\n",
    "\n",
    "    print(f\"\\n  \u2705 {parent_map} complete \u2014 best IoU: {best_iou:.4f}\")\n",
    "    return BEST_CKPT if BEST_CKPT.exists() else LATEST_CKPT\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CHECKPOINT HELPERS\n",
    "# ----------------------------\n",
    "\n",
    "def save_training_checkpoint(model, optimizer, epoch, map_name, best_iou, path):\n",
    "    inner = model.module if hasattr(model, \"module\") else model\n",
    "    ckpt = {\n",
    "        \"model_state_dict\": inner.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"map_name\": map_name,\n",
    "        \"best_iou\": best_iou,\n",
    "    }\n",
    "    torch.save(ckpt, path.with_suffix(\".train.pt\"))\n",
    "\n",
    "\n",
    "def save_inference_checkpoint(model, path):\n",
    "    inner = model.module if hasattr(model, \"module\") else model\n",
    "    torch.save(\n",
    "        {k: v.cpu() for k, v in inner.state_dict().items()},\n",
    "        path\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Training engine ready \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 \u2014 Execute Training\n",
    "Groups MAPC sub-maps (MAP1.1, MAP1.2, \u2026) by parent (MAP1, MAP2, \u2026) and trains each parent map sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover sub-map folders and group by parent map\n",
    "sub_folders = sorted([\n",
    "    d.name for d in DATA_DIR.iterdir()\n",
    "    if d.is_dir() and d.name.startswith(\"MAP\") and \".\" in d.name\n",
    "])\n",
    "\n",
    "parent_maps = []\n",
    "seen = set()\n",
    "for name in sub_folders:\n",
    "    parent = name.split(\".\")[0]     # \"MAP1.123\" -> \"MAP1\"\n",
    "    if parent not in seen:\n",
    "        seen.add(parent)\n",
    "        parent_maps.append(parent)\n",
    "parent_maps.sort()\n",
    "\n",
    "sub_counts = {p: sum(1 for n in sub_folders if n.startswith(p + \".\")) for p in parent_maps}\n",
    "print(f\"Found {len(parent_maps)} parent maps: {[f'{p} ({sub_counts[p]} sub-maps)' for p in parent_maps]}\")\n",
    "\n",
    "prev_ckpt = None\n",
    "for idx, p_name in enumerate(parent_maps):\n",
    "    print(f\"\\n\\u23f3 Training {p_name} ({sub_counts[p_name]} sub-maps)...\")\n",
    "    ckpt = train_parent_map(p_name, resume_from=prev_ckpt)\n",
    "\n",
    "    if ckpt and ckpt.exists():\n",
    "        prev_ckpt = ckpt\n",
    "        print(f\"\\u2705 {p_name} done \\u2014 checkpoint: {ckpt}\")\n",
    "    else:\n",
    "        print(f\"\\u274c {p_name} failed\")\n",
    "\n",
    "    # Ask permission before continuing to next parent map\n",
    "    if idx < len(parent_maps) - 1:\n",
    "        next_map = parent_maps[idx + 1]\n",
    "        answer = input(f\"\\n\\U0001f539 Continue to {next_map}? [yes/no]: \").strip().lower()\n",
    "        if answer not in (\"yes\", \"y\"):\n",
    "            print(f\"\\u23f9 Stopped after {p_name}. Checkpoint saved at: {prev_ckpt}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n*** LOCAL TRAINING (MAPC) COMPLETE ***\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}