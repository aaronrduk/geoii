{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "if str(NOTEBOOK_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(NOTEBOOK_DIR))\n",
        "\n",
        "DATA_DIR = Path(\"C:\\\\DATA\")\n",
        "CKPT_DIR = NOTEBOOK_DIR / \"checkpoints\"\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(NOTEBOOK_DIR / \"logs\").mkdir(exist_ok=True)\n",
        "\n",
        "# Device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"NVIDIA GPU (CUDA) \\u2705\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on CPU \\u26a0\\ufe0f\")\n",
        "\n",
        "print(f\"DATA: {DATA_DIR}  (exists={DATA_DIR.exists()})\")\n",
        "\n",
        "CONFIG = dict(\n",
        "    backbone              = \"resnet34\",  # Lighter for local test\n",
        "    pretrained            = True,\n",
        "    image_size            = 256,         # Smaller for local test\n",
        "    batch_size            = 4,\n",
        "    epochs_per_map        = 5,           # Less epochs for local test\n",
        "    learning_rate         = 2e-4,\n",
        "    weight_decay          = 1e-4,\n",
        "    num_workers           = 0,\n",
        "    mixed_precision       = True,       # CUDA AMP enabled\n",
        "    gradient_clip         = 1.0,\n",
        "    building_weight       = 1.0,\n",
        "    roof_weight           = 0.5,\n",
        "    road_weight           = 0.8,\n",
        "    waterbody_weight      = 0.8,\n",
        "    road_centerline_weight= 0.7,\n",
        "    waterbody_line_weight = 0.7,\n",
        "    waterbody_point_weight= 0.9,\n",
        "    utility_line_weight   = 0.7,\n",
        "    utility_poly_weight   = 0.8,\n",
        "    bridge_weight         = 1.0,\n",
        "    railway_weight        = 0.9,\n",
        ")\n",
        "\n",
        "TARGET_KEYS = [\n",
        "    \"building_mask\", \"road_mask\", \"road_centerline_mask\",\n",
        "    \"waterbody_mask\", \"waterbody_line_mask\", \"waterbody_point_mask\",\n",
        "    \"utility_line_mask\", \"utility_poly_mask\",\n",
        "    \"bridge_mask\", \"railway_mask\", \"roof_type_mask\",\n",
        "]\n",
        "\n",
        "print(\"\\nSetup complete \\u2713\")\n",
        "\n",
        "from models.feature_extractor import FeatureExtractor\n",
        "from models.losses import MultiTaskLoss\n",
        "from training.metrics import MetricTracker\n",
        "from data.dataset import SvamitvaDataset\n",
        "from data.augmentation import get_train_transforms\n",
        "\n",
        "def move_targets(batch):\n",
        "    return {k: batch[k].to(device) for k in TARGET_KEYS if k in batch}\n",
        "\n",
        "def build_model(load_from: Path = None):\n",
        "    m = FeatureExtractor(\n",
        "        backbone=CONFIG[\"backbone\"],\n",
        "        pretrained=CONFIG[\"pretrained\"],\n",
        "        num_roof_classes=5,\n",
        "    )\n",
        "    if load_from and load_from.exists():\n",
        "        state   = torch.load(load_from, map_location=\"cpu\", weights_only=False)\n",
        "        weights = state.get(\"model\") or state.get(\"model_state_dict\") or state\n",
        "        m.load_state_dict(weights, strict=False)\n",
        "        print(f\"Loaded: {load_from.name}\")\n",
        "    return m.to(device)\n",
        "\n",
        "\n",
        "def train_map(map_name: str, resume_from: Path = None):\n",
        "    map_dir  = DATA_DIR / map_name\n",
        "    best_out = CKPT_DIR / f\"{map_name}_best.pt\"\n",
        "    last_out = CKPT_DIR / f\"{map_name}_latest.pt\"\n",
        "\n",
        "    if not map_dir.exists():\n",
        "        print(f\"[SKIP] {map_dir} not found\")\n",
        "        return best_out if best_out.exists() else None\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"  Region     : {map_name}\")\n",
        "    print(f\"  Checkpoint : {resume_from.name if resume_from and resume_from.exists() else 'SCRATCH'}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    model_w = build_model(load_from=resume_from)\n",
        "\n",
        "    try:\n",
        "        ds = SvamitvaDataset(\n",
        "            root_dir=DATA_DIR,\n",
        "            image_size=CONFIG[\"image_size\"],\n",
        "            transform=get_train_transforms(CONFIG[\"image_size\"]),\n",
        "            mode=\"train\",\n",
        "        )\n",
        "        ds.samples = [s for s in ds.samples if s[\"map_name\"] == map_name]\n",
        "        print(f\"  Tiles (KMeans Filtered): {len(ds)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Dataset failed for {map_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    loader    = DataLoader(ds, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=False)\n",
        "    loss_fn   = MultiTaskLoss(**{k: v for k, v in CONFIG.items() if k.endswith(\"_weight\")}).to(device)\n",
        "    optimizer = torch.optim.AdamW(model_w.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG[\"epochs_per_map\"], eta_min=1e-6)\n",
        "\n",
        "    best_iou = 0.0\n",
        "    for epoch in range(1, CONFIG[\"epochs_per_map\"] + 1):\n",
        "        model_w.train()\n",
        "        tracker  = MetricTracker()\n",
        "        run_loss = 0.0\n",
        "        n_steps  = 0\n",
        "        t0       = time.time()\n",
        "\n",
        "        try:\n",
        "            for batch in loader:\n",
        "                images  = batch[\"image\"].to(device)\n",
        "                targets = move_targets(batch)\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                # NaN check the images\n",
        "                if not torch.isfinite(images).all():\n",
        "                    print(\"  [NaN SKIP] NaN in images detected\")\n",
        "                    continue\n",
        "\n",
        "                preds              = model_w(images)\n",
        "                total_loss, loss_d = loss_fn(preds, targets)\n",
        "\n",
        "                if not torch.isfinite(total_loss):\n",
        "                    print(f\"  [NaN SKIP] epoch {epoch} loss is NaN\")\n",
        "                    continue\n",
        "\n",
        "                total_loss.backward()\n",
        "                if CONFIG[\"gradient_clip\"] > 0:\n",
        "                    nn.utils.clip_grad_norm_(model_w.parameters(), CONFIG[\"gradient_clip\"])\n",
        "                optimizer.step()\n",
        "\n",
        "                run_loss += total_loss.item()\n",
        "                tracker.update(preds, targets)\n",
        "                n_steps += 1\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('\\n  [INTERRUPT] Early stop by user. Saving emergency checkpoint...')\n",
        "            if 'model_w' in locals():\n",
        "                inner = model_w.module if hasattr(model_w, 'module') else model_w\n",
        "                ckpt = {'model': inner.state_dict(), 'epoch': epoch, 'map_name': map_name, 'best_iou': best_iou, 'metrics': tracker.compute()}\n",
        "                torch.save(ckpt, last_out)\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f'\\n  [ERROR] {e}. Saving emergency checkpoint...')\n",
        "            if 'model_w' in locals():\n",
        "                inner = model_w.module if hasattr(model_w, 'module') else model_w\n",
        "                ckpt = {'model': inner.state_dict(), 'epoch': epoch, 'map_name': map_name, 'best_iou': best_iou, 'metrics': tracker.compute()}\n",
        "                torch.save(ckpt, last_out)\n",
        "            raise\n",
        "        scheduler.step()\n",
        "        m        = tracker.compute()\n",
        "        avg_loss = run_loss / max(n_steps, 1)\n",
        "        avg_iou  = m.get(\"avg_iou\", 0.0)\n",
        "\n",
        "        print(f\"  Epoch {epoch:2d}/{CONFIG['epochs_per_map']} | loss: {avg_loss:.4f} | iou: {avg_iou:.4f} | {time.time()-t0:.0f}s\")\n",
        "\n",
        "        ckpt = {\"model\": model_w.state_dict(), \"epoch\": epoch, \"map_name\": map_name, \"best_iou\": best_iou, \"metrics\": m}\n",
        "        torch.save(ckpt, last_out)\n",
        "        if avg_iou > best_iou:\n",
        "            best_iou = avg_iou\n",
        "            ckpt[\"best_iou\"] = best_iou\n",
        "            torch.save(ckpt, best_out)\n",
        "            print(f\"    \\u2192 New best! IoU = {best_iou:.4f}\")\n",
        "\n",
        "    print(f\"\\n  [DONE] {map_name}  Best IoU={best_iou:.4f}\")\n",
        "    return best_out\n",
        "\n",
        "print(\"Training engine ready \\u2713\")\n",
        "\n",
        "cpt1 = train_map(\"MAP1\", resume_from=None)\n",
        "print(\"\\n*** LOCAL TRAINING COMPLETE ***\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}