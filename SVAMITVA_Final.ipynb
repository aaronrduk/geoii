{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SVAMITVA Feature Extraction Model - Training Pipeline\n",
                "### Production-Ready Training for 10+ Tasks on DGX Server\n",
                "\n",
                "This notebook provides a well-organized and robust pipeline for training the SVAMITVA multi-task deep learning model. It is configured for the **DGX Server** environment and handles 10 unique geospatial extraction tasks.\n",
                "\n",
                "**Tasks Covered:**\n",
                "1. Building Mask\n",
                "2. Roof Type (Classification)\n",
                "3. Road Mask\n",
                "4. Road Centerline\n",
                "5. Waterbody Mask\n",
                "6. Waterbody Line\n",
                "7. Waterbody Point\n",
                "8. Utility Line\n",
                "9. Utility Polygon\n",
                "10. Bridge Mask\n",
                "11. Railway Mask"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports\n",
                "We start by initializing the environment, setting random seeds for reproducibility, and configuring the device (GPU/CPU)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import time\n",
                "import gc\n",
                "import torch\n",
                "import numpy as np\n",
                "import random\n",
                "from tqdm.auto import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "from IPython.display import clear_output\n",
                "\n",
                "# Ensure project root is in path\n",
                "sys.path.append(os.path.abspath('.'))\n",
                "\n",
                "# Local module imports\n",
                "from models.feature_extractor import FeatureExtractor\n",
                "from models.losses import MultiTaskLoss\n",
                "from data.dataset import create_dataloaders\n",
                "from training.config import TrainingConfig, get_config_from_args\n",
                "from training.metrics import MetricTracker\n",
                "from utils.checkpoint import CheckpointManager\n",
                "from utils.logging_config import setup_logging\n",
                "\n",
                "def set_seed(seed=42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "\n",
                "set_seed(42)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if device.type == 'cuda':\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration & Hyperparameters\n",
                "Update the paths below to match the DGX server directory structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DGX Server Configuration\n",
                "DATA_DIR = \"/jupyter/sods.user04/DATA/\"\n",
                "CHECKPOINT_DIR = \"/jupyter/sods.user04/svamitva_model/checkpoints/\"\n",
                "LOGS_DIR = \"/jupyter/sods.user04/svamitva_model/logs/\"\n",
                "\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "os.makedirs(LOGS_DIR, exist_ok=True)\n",
                "\n",
                "# Training Parameters\n",
                "config = TrainingConfig(\n",
                "    train_dir=DATA_DIR,\n",
                "    batch_size=8, \n",
                "    num_epochs=100,\n",
                "    lr=1e-4,\n",
                "    backbone=\"resnet50\",\n",
                "    image_size=512,\n",
                "    num_workers=0,  # DGX Jupyter best practice is 0 workers to avoid IPC errors\n",
                "    use_amp=True,   # Automatic Mixed Precision for faster DGX training\n",
                "    checkpoint_dir=CHECKPOINT_DIR\n",
                ")\n",
                "\n",
                "logger = setup_logging(os.path.join(LOGS_DIR, \"train.log\"))\n",
                "print(f\"Configured for training on data in: {DATA_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading\n",
                "Initializing the dataloaders with automatic shapefile caching (loaded once per MAP folder)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Initializing dataloaders...\")\n",
                "train_loader, val_loader = create_dataloaders(\n",
                "    train_dir=config.train_dir,\n",
                "    batch_size=config.batch_size,\n",
                "    image_size=config.image_size,\n",
                "    num_workers=config.num_workers,\n",
                "    val_split=0.15\n",
                ")\n",
                "\n",
                "print(f\"Samples: {len(train_loader.dataset)} Training, {len(val_loader.dataset)} Validation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model & Loss Initialization\n",
                "Loading the 10-head architecture with its multi-task focal loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = FeatureExtractor(backbone=config.backbone).to(device)\n",
                "criterion = MultiTaskLoss().to(device)\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4)\n",
                "scaler = torch.cuda.amp.GradScaler(enabled=config.use_amp)\n",
                "\n",
                "checkpoint_manager = CheckpointManager(CHECKPOINT_DIR)\n",
                "print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Main Training Loop\n",
                "Features include live metric tracking, best model saving, and memory protection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history = {'train_loss': [], 'val_loss': [], 'val_iou': []}\n",
                "start_epoch = 0\n",
                "\n",
                "# Optional: Resume from latest checkpoint\n",
                "checkpoint_path = checkpoint_manager.get_latest_checkpoint()\n",
                "if checkpoint_path:\n",
                "    print(f\"Found checkpoint: {checkpoint_path}\")\n",
                "    # model, optimizer, start_epoch, _ = checkpoint_manager.load(checkpoint_path, model, optimizer)\n",
                "\n",
                "for epoch in range(start_epoch, config.num_epochs):\n",
                "    model.train()\n",
                "    train_tracker = MetricTracker()\n",
                "    \n",
                "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs} [Train]\")\n",
                "    for batch_idx, (images, targets) in enumerate(pbar):\n",
                "        images = images.to(device)\n",
                "        targets = {k: v.to(device) for k, v in targets.items()}\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        with torch.cuda.amp.autocast(enabled=config.use_amp):\n",
                "            outputs = model(images)\n",
                "            loss_dict = criterion(outputs, targets)\n",
                "            total_loss = loss_dict['total_loss']\n",
                "        \n",
                "        if torch.isnan(total_loss):\n",
                "            print(\"NaN loss detected, skipping step\")\n",
                "            continue\n",
                "            \n",
                "        scaler.scale(total_loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        train_tracker.update(loss_dict, outputs, targets)\n",
                "        pbar.set_postfix({'loss': f\"{total_loss.item():.4f}\"})\n",
                "        \n",
                "    # Validation phase\n",
                "    model.eval()\n",
                "    val_tracker = MetricTracker()\n",
                "    with torch.no_grad():\n",
                "        vbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
                "        for images, targets in vbar:\n",
                "            images = images.to(device)\n",
                "            targets = {k: v.to(device) for k, v in targets.items()}\n",
                "            \n",
                "            outputs = model(images)\n",
                "            loss_dict = criterion(outputs, targets)\n",
                "            val_tracker.update(loss_dict, outputs, targets)\n",
                "            \n",
                "    # Summarize & Plot\n",
                "    epoch_train_loss = train_tracker.get_avg_loss()\n",
                "    epoch_val_loss = val_tracker.get_avg_loss()\n",
                "    epoch_val_iou = val_tracker.get_avg_iou()\n",
                "    \n",
                "    history['train_loss'].append(epoch_train_loss)\n",
                "    history['val_loss'].append(epoch_val_loss)\n",
                "    history['val_iou'].append(epoch_val_iou)\n",
                "    \n",
                "    checkpoint_manager.save(model, optimizer, epoch, epoch_val_iou)\n",
                "    \n",
                "    # Live Feedback\n",
                "    clear_output(wait=True)\n",
                "    plt.figure(figsize=(12, 5))\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.plot(history['train_loss'], label='Train Loss')\n",
                "    plt.plot(history['val_loss'], label='Val Loss')\n",
                "    plt.title('Loss History')\n",
                "    plt.legend()\n",
                "    \n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.plot(history['val_iou'], label='Val Avg IoU', color='green')\n",
                "    plt.title('Validation IoU')\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"Epoch {epoch+1} Summary:\")\n",
                "    print(f\"  Train Loss: {epoch_train_loss:.4f}\")\n",
                "    print(f\"  Val Loss:   {epoch_val_loss:.4f}\")\n",
                "    print(f\"  Val IoU:    {epoch_val_iou:.4f}\")\n",
                "\n",
                "    # Memory cleanup\n",
                "    gc.collect()\n",
                "    if device.type == 'cuda':\n",
                "        torch.cuda.empty_cache()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}